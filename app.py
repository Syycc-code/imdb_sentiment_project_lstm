# app.py (PyTorch + LSTM ç‰ˆæœ¬)
import os
import json
import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from sklearn.model_selection import train_test_split

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

st.set_page_config(page_title="IMDb The Sentiment Analysis of Movie Reviewsï¼ˆPyTorch LSTMï¼‰", layout="wide")
st.title("ğŸ¬ IMDb The Sentiment Analysis of Movie Reviewsï¼ˆPyTorch + LSTMï¼‰")

# ---------- è¾…åŠ©ï¼šæ–‡æœ¬é¢„å¤„ç† / è¯è¡¨ ----------
def simple_tokenize(text):
    # éä¸¥æ ¼åˆ†è¯ï¼šå°å†™ï¼ŒæŒ‰ç©ºç™½åˆ†è¯ï¼Œå»æ‰å‰åç©ºç™½
    return str(text).lower().strip().split()

def build_vocab(texts, num_words=10000, oov_token="<OOV>"):
    # ç»Ÿè®¡è¯é¢‘å¹¶å»ºç«‹è¯åˆ°ç´¢å¼•æ˜ å°„ï¼Œä¿ç•™æœ€å¸¸è§ num_words-1ï¼ˆç•™ä¸€ä¸ªç»™ OOVï¼‰
    freq = {}
    for t in texts:
        for w in simple_tokenize(t):
            freq[w] = freq.get(w, 0) + 1
    # æ’åº
    sorted_items = sorted(freq.items(), key=lambda x: x[1], reverse=True)
    # ä¿ç•™è¯è¡¨ï¼ˆä»1å¼€å§‹ï¼‰ï¼Œ0 ç”¨ä½œ padding
    vocab = {oov_token:1}
    idx = 2
    for w, _ in sorted_items:
        if idx >= num_words + 1:  # 1 reserved for OOV; index starts at 2
            break
        vocab[w] = idx
        idx += 1
    return vocab

def texts_to_sequences(texts, vocab, maxlen=200):
    sequences = []
    oov_idx = vocab.get("<OOV>", 1)
    for t in texts:
        seq = [vocab.get(w, oov_idx) for w in simple_tokenize(t)]
        # pad/truncate
        if len(seq) >= maxlen:
            seq = seq[:maxlen]
        else:
            seq = seq + [0] * (maxlen - len(seq))
        sequences.append(seq)
    return np.array(sequences, dtype=np.int64)

# ---------- PyTorch Dataset ----------
class ReviewDataset(Dataset):
    def __init__(self, sequences, labels):
        self.x = torch.tensor(sequences, dtype=torch.long)
        self.y = torch.tensor(labels, dtype=torch.float32)

    def __len__(self):
        return len(self.y)

    def __getitem__(self, idx):
        return self.x[idx], self.y[idx]

# ---------- æ¨¡å‹å®šä¹‰ ----------
class LSTMSentiment(nn.Module):
    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=128, num_layers=1, dropout=0.3):
        super().__init__()
        # padding index 0 è¢«ç”¨äºå¡«å……
        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)
        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim,
                            num_layers=num_layers, batch_first=True, bidirectional=False)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_dim, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # x: (batch, seq_len)
        emb = self.embedding(x)  # (batch, seq_len, emb)
        out, (hn, cn) = self.lstm(emb)  # out: (batch, seq_len, hidden)
        # å–æœ€åæ—¶é—´æ­¥è¾“å‡ºï¼ˆæˆ–ç”¨ hnï¼‰
        last = out[:, -1, :]  # (batch, hidden)
        last = self.dropout(last)
        out = self.fc(last)
        return self.sigmoid(out).squeeze(1)  # (batch,)

# ---------- åŠ è½½æ•°æ® ----------
@st.cache_data
def load_data():
    if not os.path.exists("IMDB Dataset.csv"):
        st.warning("è¯·å°† 'IMDB Dataset.csv' æ”¾åˆ°åº”ç”¨æ ¹ç›®å½•åå†è¿è¡Œï¼ˆä» Kaggle ä¸‹è½½ï¼‰ã€‚")
        return pd.DataFrame({"review": [], "sentiment": []})
    df = pd.read_csv("IMDB Dataset.csv")
    if 'review' not in df.columns or 'sentiment' not in df.columns:
        st.error("CSV æ–‡ä»¶ç¼ºå°‘ 'review' æˆ– 'sentiment' åˆ—ï¼Œè¯·ç¡®è®¤æ•°æ®æ–‡ä»¶æ ¼å¼ã€‚")
        return pd.DataFrame({"review": [], "sentiment": []})
    df = df[['review', 'sentiment']].dropna()
    df['label'] = df['sentiment'].map({'positive': 1, 'negative': 0})
    df['review_len'] = df['review'].astype(str).apply(len)
    return df

df = load_data()

# ---------- ä¾§è¾¹æ ï¼šç­›é€‰ä¸è¶…å‚ ----------
st.sidebar.header("Data Filtering and Training Settings")
min_len = int(df['review_len'].min()) if not df.empty else 0
max_len = int(df['review_len'].max()) if not df.empty else 1000
length_range = st.sidebar.slider("Length range of film reviews", min_len, max_len, (min_len, max_len))

filtered = df[(df['review_len'] >= length_range[0]) & (df['review_len'] <= length_range[1])] if not df.empty else df

sent_select = st.sidebar.multiselect("é€‰æ‹©æƒ…æ„Ÿ", options=['positive', 'negative'], default=['positive', 'negative'])
filtered = filtered[filtered['sentiment'].isin(sent_select)] if not df.empty else filtered

# è®­ç»ƒè¶…å‚æ•°
st.sidebar.markdown("---")
num_words = st.sidebar.number_input("(num_words)", min_value=2000, max_value=50000, value=10000, step=1000)
maxlen = st.sidebar.number_input("(maxlen)", min_value=50, max_value=1000, value=200, step=50)
embedding_dim = st.sidebar.selectbox("Embedding dimension", options=[50, 100, 128, 200], index=2)
hidden_dim = st.sidebar.number_input("number of LSTM hidden layers", min_value=32, max_value=512, value=128, step=32)
batch_size = st.sidebar.selectbox("Batch size", options=[32, 64, 128], index=0)
epochs = st.sidebar.number_input("Epochs", min_value=1, max_value=10, value=3)
train_on_subset = st.sidebar.checkbox("Train only on a small subset (to speed up demonstration)", value=True)
subset_size = st.sidebar.number_input("Number of subset samples (if subset is usedï¼‰", min_value=100, max_value=20000, value=2000, step=100)

st.header("Data Preview")
st.write(f"Number of samples after filtering:{len(filtered)}")
st.dataframe(filtered.sample(10) if len(filtered) >= 10 else filtered)

# ---------- EDA åŒºåŸŸ ----------
st.header(" EDAï¼šExploratory Data Analysis")
st.subheader("length distribution of reviewsï¼ˆbased on filtered dataï¼‰")
fig, ax = plt.subplots()
if not filtered.empty:
    filtered['review_len'].hist(bins=50, ax=ax)
ax.set_xlabel("length of review")
ax.set_ylabel("number of reviews")
st.pyplot(fig)

st.subheader("positive/negative word cloud")
sent_choice = st.radio("choose positive or negative", ['positive', 'negative'])
text = " ".join(filtered[filtered['sentiment'] == sent_choice]['review'].astype(str).tolist()) if not filtered.empty else ""
if text.strip():
    wc = WordCloud(width=800, height=300, background_color='white').generate(text)
    fig2, ax2 = plt.subplots(figsize=(10, 3))
    ax2.imshow(wc, interpolation='bilinear')
    ax2.axis('off')
    st.pyplot(fig2)
else:
    #use English
    st.info("no data available for the selected sentiment.")

st.subheader("sentiment distribution")
if not filtered.empty:
    vc = filtered['sentiment'].value_counts()
    fig3, ax3 = plt.subplots()
    ax3.bar(vc.index, vc.values, color=['green', 'red'])
    ax3.set_ylabel("number")
    st.pyplot(fig3)
else:
    st.info("no data available after filtering.")

# ---------- æ¨¡å‹è®­ç»ƒ / åŠ è½½ / é¢„æµ‹ ----------
st.header("LSTM model training and prediction")

model_path = "pytorch_lstm_model.pt"
vocab_path = "vocab.json"

use_existing = st.checkbox("use model that exists", value=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
st.write(f"deviceï¼š{device}")

model = None
vocab = None

if use_existing and os.path.exists(model_path) and os.path.exists(vocab_path):
    try:
        with open(vocab_path, 'r', encoding='utf-8') as f:
            vocab = json.load(f)
        # vocab size = max index + 1 or num_words + 2 etc. We'll use num_words+2 as safe upper bound
        vocab_size = max(list(vocab.values())) + 1
        model = LSTMSentiment(vocab_size=vocab_size, embedding_dim=embedding_dim, hidden_dim=hidden_dim).to(device)
        model.load_state_dict(torch.load(model_path, map_location=device))
        model.eval()
        st.success("The existing model and vocabulary have been loaded.")
    except Exception as e:
        st.error(f"failedï¼š{e}")
        model = None
        vocab = None

# è®­ç»ƒè§¦å‘
if st.button("training new model"):
    if df.empty:
        st.error("data is empty, cannot train model.")
    else:
        # é€‰æ‹©æ•°æ®ï¼ˆå¯ç”¨å…¨é‡æˆ–å­é›†ï¼‰
        data = df.sample(n=subset_size, random_state=42) if train_on_subset else df
        texts = data['review'].astype(str).tolist()
        labels = data['label'].astype(int).tolist()
        st.info(f"begin to trainï¼šnum of input={len(texts)}ï¼Œlength of vocabulary={num_words}ï¼Œlength of sequence={maxlen}")

        # build vocab
        vocab = build_vocab(texts, num_words=num_words)
        # ä¿å­˜è¯è¡¨
        with open(vocab_path, 'w', encoding='utf-8') as f:
            json.dump(vocab, f, ensure_ascii=False)

        sequences = texts_to_sequences(texts, vocab, maxlen=maxlen)
        X_train, X_val, y_train, y_val = train_test_split(sequences, labels, test_size=0.15, random_state=42)

        train_ds = ReviewDataset(X_train, y_train)
        val_ds = ReviewDataset(X_val, y_val)
        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)

        vocab_size = max(list(vocab.values())) + 1
        model = LSTMSentiment(vocab_size=vocab_size, embedding_dim=embedding_dim, hidden_dim=hidden_dim).to(device)
        criterion = nn.BCELoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

        best_val_loss = float('inf')
        patience = 2
        wait = 0
        history = {"train_loss": [], "val_loss": []}

        for epoch in range(1, epochs + 1):
            model.train()
            running_loss = 0.0
            for xb, yb in train_loader:
                xb = xb.to(device)
                yb = yb.to(device)
                optimizer.zero_grad()
                preds = model(xb)
                loss = criterion(preds, yb)
                loss.backward()
                optimizer.step()
                running_loss += loss.item() * xb.size(0)
            train_loss = running_loss / len(train_loader.dataset)
            # validation
            model.eval()
            val_loss = 0.0
            with torch.no_grad():
                for xb, yb in val_loader:
                    xb = xb.to(device)
                    yb = yb.to(device)
                    preds = model(xb)
                    loss = criterion(preds, yb)
                    val_loss += loss.item() * xb.size(0)
            val_loss = val_loss / len(val_loader.dataset)
            history["train_loss"].append(train_loss)
            history["val_loss"].append(val_loss)
            st.write(f"Epoch {epoch}/{epochs} â€” train_loss: {train_loss:.4f} â€” val_loss: {val_loss:.4f}")

            # æ—©åœ
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                wait = 0
                # ä¿å­˜æ¨¡å‹
                torch.save(model.state_dict(), model_path)
                st.write("best model saved.")
            else:
                wait += 1
                if wait >= patience:
                    st.write("Early stopping triggered.")
                    break

        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        fig_hist, ax_hist = plt.subplots()
        ax_hist.plot(history["train_loss"], label="train_loss")
        ax_hist.plot(history["val_loss"], label="val_loss")
        ax_hist.set_xlabel("epoch")
        ax_hist.set_ylabel("loss")
        ax_hist.legend()
        st.pyplot(fig_hist)

        st.success("Model training completed.")

# ---------- å•æ¡æ–‡æœ¬é¢„æµ‹ ----------
st.subheader("Single Text Prediction")
input_text = st.text_area("please enter the movie review text for sentiment prediction:", height=150)
if st.button("Predict Sentiment"):
    if input_text.strip() == "":
        st.warning("please enter valid text.")
    else:
        if model is None or vocab is None:
            st.error("No model available. Please train a new model or load an existing one.")
        else:
            seq = texts_to_sequences([input_text], vocab, maxlen=maxlen)
            x_tensor = torch.tensor(seq, dtype=torch.long).to(device)
            model.eval()
            with torch.no_grad():
                pred = model(x_tensor)
                prob = float(pred.cpu().numpy()[0])
                label = "Positive" if prob >= 0.5 else "Negative"
                st.success(f"predictionï¼š{label}ï¼ˆprobability={prob:.3f}ï¼‰")
